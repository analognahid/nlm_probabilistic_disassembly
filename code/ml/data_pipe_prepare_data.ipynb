{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d3e234-741d-493b-89f7-f00b39b6ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "import torch\n",
    "\n",
    "accelerator = Accelerator()\n",
    "accelerator.state.num_processes = 3  # For 3 GPUs\n",
    "device = accelerator.device\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a994e3c-4bb4-4196-840d-3c78c182596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys, json,re,pickle\n",
    "import magic, hashlib,  traceback ,ntpath, collections ,lief\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "import torch.nn as nn\n",
    "\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from transformers import AdamW,AutoTokenizer\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score,f1_score, confusion_matrix,mean_squared_error, mean_absolute_error, r2_score\n",
    "from numpy import *\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f654b05f-145b-405f-b917-6bd69545e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_path = '/home/raisul/DATA/x86_pe_msvc_O2_static/'\n",
    "bin_files = [ os.path.join(os.path.join(bin_path, f),f+'.exe' )  for f in os.listdir(bin_path)][0:20]\n",
    "ground_truth_path ='/home/raisul/ANALYSED_DATA/ghidra_x86_pe_msvc_O2_static/'\n",
    "MODEL_SAVE_PATH= '/home/raisul/probabilistic_disassembly/models/'\n",
    "EXPERIMENT_NAME = 'prototype_pe_debug'\n",
    "MAX_TOKEN_SIZE = 120\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "VOCAB_SIZE = 500\n",
    "BATCH_SIZE = 600\n",
    "BIN_FILE_TYPE = 'PE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af8e984-c7ac-4ce3-9978-2f8128e992ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_num_with_word(input_string , replace_dict):\n",
    "    def num_to_word(match):\n",
    "        number = int( match.group(0))\n",
    "        return num2words(replace_dict[number]).replace(' ','').replace('-',\"\")\n",
    "    result_string = re.sub(r'\\b\\d+\\b', num_to_word, input_string)\n",
    "    return result_string\n",
    "\n",
    "\n",
    "\n",
    "def replace_hex_with_decimal(input_string):\n",
    "    # Regular expression to find hexadecimal numbers prefixed with \"0x\" or \"0X\"\n",
    "    hex_pattern = r'0[xX][0-9a-fA-F]+'\n",
    "    \n",
    "    # Function to convert each found hex number to decimal\n",
    "    def hex_to_decimal(match):\n",
    "        hex_value = match.group(0)  # Extract the matched hex number\n",
    "        decimal_value = str(int(hex_value, 16))  # Convert hex to decimal\n",
    "        return decimal_value\n",
    "    # Substitute all hex numbers in the string with their decimal equivalents\n",
    "    result_string = re.sub(hex_pattern, hex_to_decimal, input_string)\n",
    "    return result_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d01dfce-edb1-4a0a-a0a8-fbaf3fac12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ground_truth_ghidra(exe_path, text_section_offset , text_section_len):\n",
    "\n",
    "    text_sextion_end = text_section_offset + text_section_len\n",
    "    \n",
    "    elf_file_name = os.path.basename(exe_path)\n",
    "    ghidra_file_path = os.path.join(ground_truth_path, elf_file_name.split('.')[0]) + '.json'\n",
    "    \n",
    "    with open(ghidra_file_path, \"r\") as file:\n",
    "        ghidra_data = json.load(file)\n",
    "\n",
    "    ground_truth_offsets = list(ghidra_data.keys())\n",
    "\n",
    "    ground_truth_offsets = [int(i) for i in ground_truth_offsets]\n",
    "    ground_truth_offsets = [x for x in ground_truth_offsets if text_section_offset <= x <= text_sextion_end]\n",
    "    ground_truth_offsets.sort()\n",
    "    return ground_truth_offsets\n",
    "\n",
    "\n",
    "\n",
    "def find_data_in_textsection(ground_truth_offsets , text_section_offset , text_section_len, offset_inst_dict):\n",
    "    data_offsets = []\n",
    "    for i in range(1, len(ground_truth_offsets)-1):\n",
    "        distance = ground_truth_offsets[i+1] - ground_truth_offsets[i]\n",
    "\n",
    "        inst_len = offset_inst_dict[ground_truth_offsets[i]].size \n",
    "        \n",
    "        if distance!=inst_len:\n",
    "            # print('offset_ranges[i]: ',ground_truth_offsets[i] , 'offset_ranges[i-1]: ',ground_truth_offsets[i-1], ' inst_len: ',inst_len  )\n",
    "            \n",
    "            # print(ground_truth_offsets[i],' ' ,hex(ground_truth_offsets[i]) , offset_inst_dict[ground_truth_offsets[i]], ' len',offset_inst_dict[ground_truth_offsets[i]].size )\n",
    "            # print(\"\\nByte GAP ###### \",distance ,' Missing bytes: ', distance - inst_len)\n",
    "            \n",
    "            for j in range( ground_truth_offsets[i] +inst_len , ground_truth_offsets[i+1]  ):\n",
    "                data_offsets.append(j)\n",
    "                # if offset_inst_dict[j]:\n",
    "                #     print(\"# \",j, offset_inst_dict[j].mnemonic, offset_inst_dict[j].op_str , 'inst len:',offset_inst_dict[j].size )\n",
    "                # else:\n",
    "                #     print(\"# \",j, \" invalid \")\n",
    "            # print('\\n')\n",
    "        else:\n",
    "            # print(ground_truth_offsets[i],' ', hex(ground_truth_offsets[i]) , offset_inst_dict[ground_truth_offsets[i]].mnemonic,offset_inst_dict[ground_truth_offsets[i]].op_str ,' len',offset_inst_dict[ground_truth_offsets[i]].size)\n",
    "            pass\n",
    "    return data_offsets\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2f8ec7-72bd-4fd8-aef3-90ef4e0fcaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: [Errno 2] No such file or directory: '/home/raisul/ANALYSED_DATA/ghidra_x86_pe_msvc_O2_static/ff556502c0d51a4d7f46d9408ed320a9.json' /home/raisul/DATA/x86_pe_msvc_O2_static/ff556502c0d51a4d7f46d9408ed320a9/ff556502c0d51a4d7f46d9408ed320a9.exe\n",
      "An error occurred: [Errno 2] No such file or directory: '/home/raisul/ANALYSED_DATA/ghidra_x86_pe_msvc_O2_static/ec768bdbfb5ade598a230b74d7719d04.json' /home/raisul/DATA/x86_pe_msvc_O2_static/ec768bdbfb5ade598a230b74d7719d04/ec768bdbfb5ade598a230b74d7719d04.exe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linear_sweep(offset_inst , target_offset):\n",
    "    inst_sequence = ''\n",
    "    address_list = []\n",
    "    \n",
    "    current_offset = target_offset\n",
    "    for q in range(MAX_SEQUENCE_LENGTH):\n",
    "\n",
    "        if current_offset in offset_inst: #if end of text section\n",
    "            current_instruction = offset_inst[current_offset]\n",
    "            if current_instruction is None:\n",
    "                return  None\n",
    "                \n",
    "            current_offset = current_offset + current_instruction.size\n",
    "            inst_sequence+= str( hex(current_instruction.address)) +\" \"+ current_instruction.mnemonic +' '+ current_instruction.op_str+ ' ; ' \n",
    "            address_list.append(current_instruction.address)\n",
    "            \n",
    "            \n",
    "            if current_instruction.mnemonic in [\"ret\", \"jmp\"]: #break linear sweep\n",
    "                break\n",
    "                \n",
    "\n",
    "    return inst_sequence, address_list\n",
    "\n",
    "\n",
    "SEQUENCES = []\n",
    "LABELS     = []\n",
    "\n",
    "for bin_file_path in bin_files:\n",
    "\n",
    "    \n",
    "    md = Cs(CS_ARCH_X86, CS_MODE_64)\n",
    "    md.detail = True\n",
    "    offset_inst = {}\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    try:\n",
    "        with open(bin_file_path, 'rb') as f:\n",
    "            if BIN_FILE_TYPE == \"ELF\":\n",
    "                elffile = ELFFile(f)\n",
    "                textSection = elffile.get_section_by_name('.text').data()\n",
    "                text_section_offset = elffile.get_section_by_name('.text')['sh_offset']\n",
    "            elif BIN_FILE_TYPE == \"PE\":\n",
    "                pe_file = lief.parse(bin_file_path)\n",
    "                text_section = pe_file.get_section(\".text\")\n",
    "                text_section_offset = text_section.pointerto_raw_data\n",
    "                textSection = bytes(text_section.content)\n",
    "            \n",
    "            ground_truth_offsets = get_ground_truth_ghidra(bin_file_path, text_section_offset, len(textSection))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e ,bin_file_path)\n",
    "        continue\n",
    "\n",
    "    for byte_index in range(len(textSection)):\n",
    "        try:    \n",
    "\n",
    "            instruction = next(md.disasm(textSection[byte_index: byte_index+15 ], text_section_offset + byte_index ), None)\n",
    "            offset_inst[text_section_offset+byte_index] = instruction\n",
    "            \n",
    "            # if instruction:\n",
    "            #     print(\"%d:\\t%s\\t%s _\\t%x\" %(int(instruction.address), instruction.mnemonic, instruction.op_str, instruction.size))\n",
    "            # else:\n",
    "            #     print(\"%d:\\t%s \" % (text_section_offset + byte_index  , 'invalid instruction') )\n",
    "                \n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(traceback.print_exc() )\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "    \n",
    "    offset_inst_dict = collections.OrderedDict(sorted(offset_inst.items()))\n",
    "\n",
    "    DATA_OFFSETS = find_data_in_textsection(ground_truth_offsets , text_section_offset , len(textSection) , offset_inst)\n",
    "\n",
    "\n",
    "    \n",
    "    for byte_offset in range(text_section_offset, text_section_offset+len(textSection)):\n",
    "        return_value = linear_sweep(offset_inst_dict ,  byte_offset )\n",
    "        if return_value== None:\n",
    "            continue\n",
    "        inst_seq, inst_addresses = return_value \n",
    "        ###################################################################\n",
    "        ## number to words\n",
    "        disassembly_decimal = replace_hex_with_decimal(inst_seq)\n",
    "\n",
    "        #num to words all\n",
    "        numbers = [int(s) for s in re.findall(r'\\b\\d+\\b', disassembly_decimal)]\n",
    "        numbers = sorted(set(numbers) , reverse=True)\n",
    "        number_word_dict = {}\n",
    "        \n",
    "        for ix,n in enumerate(numbers):\n",
    "            number_word_dict[n] = len(numbers)-1 -ix\n",
    "\n",
    "        disassembly_num_to_words = replace_num_with_word(disassembly_decimal , number_word_dict)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        ###########################################################################\n",
    "        \n",
    "        \n",
    "        SEQUENCES.append(disassembly_num_to_words)\n",
    "        if byte_offset in ground_truth_offsets:\n",
    "            LABELS.append(float(1))\n",
    "        else:\n",
    "            LABELS.append(float(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7fa81ca-66f4-4bd8-ad3e-aac8e4ea7d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358774 358774\n",
      "330961 27813\n"
     ]
    }
   ],
   "source": [
    "print(len(SEQUENCES) , len(LABELS))\n",
    "print(LABELS.count(0), LABELS.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0307133-f04b-43dc-b48f-1a71d755ce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "from transformers import BertTokenizer,BertForSequenceClassification\n",
    "\n",
    "# If using a character-level tokenizer for sequences like DNA/Protein:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")#BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = tokenizer.train_new_from_iterator(SEQUENCES, VOCAB_SIZE)\n",
    "\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH +\"bert_tokenizer\" )\n",
    "\n",
    "\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     'bert-base-uncased',\n",
    "#     num_labels=1  \n",
    "# )\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=1  \n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(VOCAB_SIZE)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optim = AdamW( model.parameters() , lr=1e-5, eps = 1e-6, betas=(0.9,0.98), weight_decay=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2517e6b5-4b9b-4678-b78d-1243f0fc471c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero int3  ; one int3  ; two int3  ; three int3  ; four int3  ; five jmp six ; '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQUENCES[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b4129cd-e356-4327-9e23-72beed13b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Tokenize the \n",
    "        tokenized_text = (self.tokenizer(text , max_length= MAX_TOKEN_SIZE,padding='max_length', truncation=True , return_tensors='pt')).to(device)\n",
    "        \n",
    "        return tokenized_text, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fa283cd-ae7b-4b14-aef8-d197ca0ad338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('starting')\n",
    "dataset = BinaryDataset(SEQUENCES, LABELS, tokenizer)\n",
    "# with open(MODEL_SAVE_PATH+'training_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(dataset, f)\n",
    "\n",
    "# # Load from file\n",
    "# with open(MODEL_SAVE_PATH+'training_data.pkl', 'rb') as f:\n",
    "#     loaded_data = pickle.load(f)\n",
    "\n",
    "# print(len(loaded_data))  # Output: {'name': 'Alice', 'age': 25, 'city': 'New York'}\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "validation_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset  = torch.utils.data.Subset(dataset, range(train_size))\n",
    "validation_dataset = torch.utils.data.Subset(dataset, range(train_size , len(dataset)))\n",
    "\n",
    "# train_dataset, validation_dataset = torch.utils.data.random_split(dataset, [train_size, validation_size] , generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7802808-ad53-44ec-9468-aff785cfa0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE ,shuffle=False) \n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d607f76f-d318-4baa-a082-6c52399d0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optim, train_loader,validation_loader = accelerator.prepare(model, optim, train_loader,validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ee3fd5-8c3f-4ba2-9540-52d470e5c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model ,data_loop, is_training = False):\n",
    "    \n",
    "    prediction_s, ground_truth_s = [], []\n",
    "    losses = []\n",
    "\n",
    "    for N,batch in enumerate(data_loop):\n",
    "        # Forward pass\n",
    "        if is_training == True:\n",
    "            optim.zero_grad()\n",
    "        \n",
    "        batch_input, batch_labels = batch\n",
    "        if len(batch_labels)<BATCH_SIZE:\n",
    "            continue\n",
    "            \n",
    "        batch_input_ids= batch_input['input_ids']\n",
    "        batch_attention_mask=batch_input['attention_mask']\n",
    "        batch_token_type_ids =batch_input['token_type_ids']\n",
    "        \n",
    "        outputs = model(input_ids=batch_input_ids.squeeze(),\n",
    "                        attention_mask=batch_attention_mask.squeeze(),\n",
    "                        token_type_ids=batch_token_type_ids.squeeze(),\n",
    "                        labels=batch_labels.float() )\n",
    "        \n",
    "#\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = logits.squeeze()\n",
    "        # print(logits ,predictions )\n",
    "\n",
    "        prediction_s.extend(predictions.detach().cpu().numpy().flatten())\n",
    "        ground_truth_s.extend(batch_labels.detach().cpu().numpy().flatten())\n",
    "\n",
    "\n",
    "        if is_training == True:\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        data_loop.set_description(f'Epoch {ecpoch}')\n",
    "        data_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    mse = mean_squared_error(ground_truth_s, prediction_s)\n",
    "    rmse = sqrt(mse)\n",
    "    mae = mean_absolute_error(ground_truth_s, prediction_s)\n",
    "    r2 = r2_score(ground_truth_s, prediction_s)\n",
    "    \n",
    "\n",
    "    metrices = {'MSE':mse ,\n",
    "                      'RMSE':rmse, \n",
    "                      'MAE':mae, \n",
    "                      'RÂ²':r2,\n",
    "                      'loss': (sum(losses) / len(losses))}\n",
    "    return metrices , prediction_s, ground_truth_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bc064-7683-4047-b197-bd4e015bd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "global_metrices = []\n",
    "v_global_metrices = []\n",
    "\n",
    "\n",
    "for ecpoch in range(EPOCHS):\n",
    "    \n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    model.train()\n",
    "    metrices,prediction_s, ground_truth_s  = training_loop(model ,train_loop, is_training = True)\n",
    "    global_metrices.append(metrices)\n",
    "    print(\"Training metrices \",metrices)\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        validation_loop = tqdm(validation_loader, leave=True)\n",
    "        v_metrices, v_prediction_s, v_ground_truth_s  = training_loop(model ,validation_loop, is_training = False)\n",
    "\n",
    "\n",
    "        # demo_len =10000\n",
    "        # for i in range(minimum(demo_len , len(v_prediction_s) )):\n",
    "\n",
    "        #     print('\\n')\n",
    "        #     sequence_text = validation_sequences[ i ]\n",
    "        #     print( v_prediction_s[i], v_ground_truth_s[i] , '\\n' , sequence_text)\n",
    "\n",
    "\n",
    "           \n",
    "        print( 'v_metrices: ',v_metrices )\n",
    "        v_global_metrices.append(v_metrices)\n",
    "        \n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(MODEL_SAVE_PATH + EXPERIMENT_NAME)\n",
    "\n",
    "        print('SAVING MODEL @ ',MODEL_SAVE_PATH +EXPERIMENT_NAME)\n",
    "        unwrapped_model.save_pretrained(MODEL_SAVE_PATH +EXPERIMENT_NAME)\n",
    "        print('saved')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b6109-cdd3-4af3-bc42-4de303dd5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a526c-2306-41a8-a629-0be561f7ecef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
