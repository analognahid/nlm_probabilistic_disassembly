{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d3e234-741d-493b-89f7-f00b39b6ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "import torch\n",
    "\n",
    "accelerator = Accelerator()\n",
    "accelerator.state.num_processes = 3  # For 3 GPUs\n",
    "device = accelerator.device\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a994e3c-4bb4-4196-840d-3c78c182596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys, json,re\n",
    "import magic, hashlib,  traceback ,ntpath, collections ,lief\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "import torch.nn as nn\n",
    "import lief\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from transformers import AdamW,AutoTokenizer\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score,f1_score, confusion_matrix,mean_squared_error, mean_absolute_error, r2_score\n",
    "from numpy import *\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f654b05f-145b-405f-b917-6bd69545e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_FILE_TYPE = 'PE' #or ELF\n",
    "bin_path = '/home/raisul/DATA/temp/x86_pe_msvc_O2_static/'\n",
    "bin_files = [os.path.join(bin_path, f) for f in os.listdir(bin_path) if f.endswith(\".exe\")][0:10]\n",
    "ground_truth_path ='/home/raisul/DATA/temp/ghidra_x86_pe_msvc_O2_debug/'  \n",
    "MODEL_SAVE_PATH= '/home/raisul/probabilistic_disassembly/models/'\n",
    "EXPERIMENT_NAME = 'prototype_pe_small'\n",
    "MAX_TOKEN_SIZE = 120\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "VOCAB_SIZE = 500\n",
    "BATCH_SIZE = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af8e984-c7ac-4ce3-9978-2f8128e992ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_num_with_word(input_string , replace_dict):\n",
    "    def num_to_word(match):\n",
    "        number = int( match.group(0))\n",
    "        return num2words(replace_dict[number]).replace(' ','').replace('-',\"\")\n",
    "    result_string = re.sub(r'\\b\\d+\\b', num_to_word, input_string)\n",
    "    return result_string\n",
    "\n",
    "\n",
    "\n",
    "def replace_hex_with_decimal(input_string):\n",
    "    # Regular expression to find hexadecimal numbers prefixed with \"0x\" or \"0X\"\n",
    "    hex_pattern = r'0[xX][0-9a-fA-F]+'\n",
    "    \n",
    "    # Function to convert each found hex number to decimal\n",
    "    def hex_to_decimal(match):\n",
    "        hex_value = match.group(0)  # Extract the matched hex number\n",
    "        decimal_value = str(int(hex_value, 16))  # Convert hex to decimal\n",
    "        return decimal_value\n",
    "    # Substitute all hex numbers in the string with their decimal equivalents\n",
    "    result_string = re.sub(hex_pattern, hex_to_decimal, input_string)\n",
    "    return result_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d01dfce-edb1-4a0a-a0a8-fbaf3fac12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ground_truth_ghidra(exe_path, text_section_offset , text_section_len):\n",
    "\n",
    "    text_sextion_end = text_section_offset + text_section_len\n",
    "    \n",
    "    elf_file_name = os.path.basename(exe_path)\n",
    "    ghidra_file_path = os.path.join(ground_truth_path, elf_file_name.split('.')[0]) + '.json'\n",
    "    \n",
    "    with open(ghidra_file_path, \"r\") as file:\n",
    "        ghidra_data = json.load(file)\n",
    "\n",
    "    ground_truth_offsets = list(ghidra_data.keys())\n",
    "\n",
    "    ground_truth_offsets = [int(i) for i in ground_truth_offsets]\n",
    "    ground_truth_offsets = [x for x in ground_truth_offsets if text_section_offset <= x <= text_sextion_end]\n",
    "    ground_truth_offsets.sort()\n",
    "    return ground_truth_offsets\n",
    "\n",
    "\n",
    "\n",
    "def find_data_in_textsection(ground_truth_offsets , text_section_offset , text_section_len, offset_inst_dict):\n",
    "    data_offsets = []\n",
    "    for i in range(1, len(ground_truth_offsets)-1):\n",
    "        distance = ground_truth_offsets[i+1] - ground_truth_offsets[i]\n",
    "\n",
    "        inst_len = offset_inst_dict[ground_truth_offsets[i]].size \n",
    "        \n",
    "        if distance!=inst_len:\n",
    "            # print('offset_ranges[i]: ',ground_truth_offsets[i] , 'offset_ranges[i-1]: ',ground_truth_offsets[i-1], ' inst_len: ',inst_len  )\n",
    "            \n",
    "            # print(ground_truth_offsets[i],' ' ,hex(ground_truth_offsets[i]) , offset_inst_dict[ground_truth_offsets[i]], ' len',offset_inst_dict[ground_truth_offsets[i]].size )\n",
    "            # print(\"\\nByte GAP ###### \",distance ,' Missing bytes: ', distance - inst_len)\n",
    "            \n",
    "            for j in range( ground_truth_offsets[i] +inst_len , ground_truth_offsets[i+1]  ):\n",
    "                data_offsets.append(j)\n",
    "                # if offset_inst_dict[j]:\n",
    "                #     print(\"# \",j, offset_inst_dict[j].mnemonic, offset_inst_dict[j].op_str , 'inst len:',offset_inst_dict[j].size )\n",
    "                # else:\n",
    "                #     print(\"# \",j, \" invalid \")\n",
    "            # print('\\n')\n",
    "        else:\n",
    "            # print(ground_truth_offsets[i],' ', hex(ground_truth_offsets[i]) , offset_inst_dict[ground_truth_offsets[i]].mnemonic,offset_inst_dict[ground_truth_offsets[i]].op_str ,' len',offset_inst_dict[ground_truth_offsets[i]].size)\n",
    "            pass\n",
    "    return data_offsets\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2f8ec7-72bd-4fd8-aef3-90ef4e0fcaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: [Errno 2] No such file or directory: '/home/raisul/DATA/temp/ghidra_x86_pe_msvc_O2_debug/021e7e0cf652921f603df7e2790e59bb.json' /home/raisul/DATA/temp/x86_pe_msvc_O2_static/021e7e0cf652921f603df7e2790e59bb.exe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linear_sweep(offset_inst , target_offset):\n",
    "    inst_sequence = ''\n",
    "    address_list = []\n",
    "    \n",
    "    current_offset = target_offset\n",
    "    for q in range(MAX_SEQUENCE_LENGTH):\n",
    "\n",
    "        if current_offset in offset_inst: #if end of text section\n",
    "            current_instruction = offset_inst[current_offset]\n",
    "            if current_instruction is None:\n",
    "                return  None\n",
    "                \n",
    "            current_offset = current_offset + current_instruction.size\n",
    "            inst_sequence+= str( hex(current_instruction.address)) +\" \"+ current_instruction.mnemonic +' '+ current_instruction.op_str+ ' ; ' \n",
    "            address_list.append(current_instruction.address)\n",
    "            \n",
    "            if current_instruction.mnemonic in [\"ret\", \"jmp\"]: #break linear sweep\n",
    "                break\n",
    "                \n",
    "\n",
    "    return inst_sequence, address_list\n",
    "\n",
    "\n",
    "SEQUENCES = []\n",
    "LABELS     = []\n",
    "\n",
    "for bin_file_path in bin_files:\n",
    "\n",
    "    \n",
    "    md = Cs(CS_ARCH_X86, CS_MODE_64)\n",
    "    md.detail = True\n",
    "    offset_inst = {}\n",
    "\n",
    "    \n",
    "    with open(bin_file_path, 'rb') as f:\n",
    "\n",
    "        try:\n",
    "            if BIN_FILE_TYPE == \"ELF\":\n",
    "                elffile = ELFFile(f)\n",
    "                textSection = elffile.get_section_by_name('.text').data()\n",
    "                text_section_offset = elffile.get_section_by_name('.text')['sh_offset']\n",
    "              \n",
    "            elif BIN_FILE_TYPE == \"PE\":\n",
    "\n",
    "                        \n",
    "                pe_file = lief.parse(bin_file_path)\n",
    "                text_section = pe_file.get_section(\".text\")\n",
    "                text_section_offset = text_section.pointerto_raw_data\n",
    "                textSection = bytes(text_section.content)\n",
    "                \n",
    "            ground_truth_offsets = get_ground_truth_ghidra(bin_file_path, text_section_offset , len(textSection))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e ,bin_file_path)\n",
    "            continue\n",
    "\n",
    "    for byte_index in range(len(textSection)):\n",
    "        try:    \n",
    "\n",
    "            instruction = next(md.disasm(textSection[byte_index: byte_index+15 ], text_section_offset + byte_index ), None)\n",
    "            offset_inst[text_section_offset+byte_index] = instruction\n",
    "            \n",
    "            # if instruction:\n",
    "            #     print(\"%d:\\t%s\\t%s _\\t%x\" %(int(instruction.address), instruction.mnemonic, instruction.op_str, instruction.size))\n",
    "            # else:\n",
    "            #     print(\"%d:\\t%s \" % (text_section_offset + byte_index  , 'invalid instruction') )\n",
    "                \n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(traceback.print_exc() )\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "    \n",
    "    offset_inst_dict = collections.OrderedDict(sorted(offset_inst.items()))\n",
    "\n",
    "    DATA_OFFSETS = find_data_in_textsection(ground_truth_offsets , text_section_offset , len(textSection) , offset_inst)\n",
    "\n",
    "\n",
    "    \n",
    "    for byte_offset in range(text_section_offset, text_section_offset+len(textSection)):\n",
    "        return_value = linear_sweep(offset_inst_dict ,  byte_offset )\n",
    "        if return_value== None:\n",
    "            continue\n",
    "        inst_seq, inst_addresses = return_value \n",
    "        ###################################################################\n",
    "        ## number to words\n",
    "        disassembly_decimal = replace_hex_with_decimal(inst_seq)\n",
    "\n",
    "        #num to words all\n",
    "        numbers = [int(s) for s in re.findall(r'\\b\\d+\\b', disassembly_decimal)]\n",
    "        numbers = sorted(set(numbers) , reverse=True)\n",
    "        number_word_dict = {}\n",
    "        \n",
    "        for ix,n in enumerate(numbers):\n",
    "            number_word_dict[n] = len(numbers)-1 -ix\n",
    "\n",
    "        disassembly_num_to_words = replace_num_with_word(disassembly_decimal , number_word_dict)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        ###########################################################################\n",
    "        \n",
    "        \n",
    "        SEQUENCES.append(disassembly_num_to_words)\n",
    "        if byte_offset in ground_truth_offsets:\n",
    "            LABELS.append(float(1))\n",
    "        else:\n",
    "            LABELS.append(float(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7fa81ca-66f4-4bd8-ad3e-aac8e4ea7d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187789 187789\n",
      "171592 16197\n"
     ]
    }
   ],
   "source": [
    "print(len(SEQUENCES) , len(LABELS))\n",
    "print(LABELS.count(0), LABELS.count(1))\n",
    "# for j in range(10000):\n",
    "#     if LABELS[j]==0:\n",
    "#         print(LABELS[j] , ' > ' , SEQUENCES[j]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0307133-f04b-43dc-b48f-1a71d755ce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "from transformers import BertTokenizer,BertForSequenceClassification\n",
    "\n",
    "# If using a character-level tokenizer for sequences like DNA/Protein:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")#BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = tokenizer.train_new_from_iterator(SEQUENCES, VOCAB_SIZE)\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=1  \n",
    ")\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     MODEL_SAVE_PATH +EXPERIMENT_NAME,\n",
    "#     num_labels=1  \n",
    "# )\n",
    "\n",
    "model.resize_token_embeddings(VOCAB_SIZE)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optim = AdamW( model.parameters() , lr=1e-5, eps = 1e-6, betas=(0.9,0.98), weight_decay=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2517e6b5-4b9b-4678-b78d-1243f0fc471c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero int3  ; one int3  ; two int3  ; three int3  ; four int3  ; five jmp six ; '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQUENCES[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b4129cd-e356-4327-9e23-72beed13b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Tokenize the \n",
    "        tokenized_text = (self.tokenizer(text , max_length= MAX_TOKEN_SIZE,padding='max_length', truncation=True , return_tensors='pt')).to(device)\n",
    "        \n",
    "        return tokenized_text, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fa283cd-ae7b-4b14-aef8-d197ca0ad338",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BinaryDataset(SEQUENCES, LABELS,tokenizer)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "validation_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset  = torch.utils.data.Subset(dataset, range(train_size))\n",
    "validation_dataset = torch.utils.data.Subset(dataset, range(train_size , len(dataset)))\n",
    "\n",
    "# train_dataset, validation_dataset = torch.utils.data.random_split(dataset, [train_size, validation_size] , generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f4ecfe2-f33d-42e8-aba0-f9e506fd401c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150231, 37558)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(train_dataset) , len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62c119b9-3fb7-4264-be17-e50d77f81283",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE ,shuffle=False) \n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f3296ff-458f-48d5-be35-85e6dcd7a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optim, train_loader,validation_loader = accelerator.prepare(model, optim, train_loader,validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fff2a619-6a06-4774-8ec7-bad9a55e8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model ,data_loop, is_training = False):\n",
    "    \n",
    "    prediction_s, ground_truth_s = [], []\n",
    "    losses = []\n",
    "\n",
    "    for N,batch in enumerate(data_loop):\n",
    "        # Forward pass\n",
    "        if is_training == True:\n",
    "            optim.zero_grad()\n",
    "        \n",
    "        batch_input, batch_labels = batch\n",
    "        if len(batch_labels)<BATCH_SIZE:\n",
    "            continue\n",
    "            \n",
    "        batch_input_ids= batch_input['input_ids']\n",
    "        batch_attention_mask=batch_input['attention_mask']\n",
    "        batch_token_type_ids =batch_input['token_type_ids']\n",
    "        \n",
    "        outputs = model(input_ids=batch_input_ids.squeeze(),\n",
    "                        attention_mask=batch_attention_mask.squeeze(),\n",
    "                        token_type_ids=batch_token_type_ids.squeeze(),\n",
    "                        labels=batch_labels.float() )\n",
    "        \n",
    "#\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = logits.squeeze()\n",
    "        # print(logits ,predictions )\n",
    "\n",
    "        prediction_s.extend(predictions.detach().cpu().numpy().flatten())\n",
    "        ground_truth_s.extend(batch_labels.detach().cpu().numpy().flatten())\n",
    "\n",
    "\n",
    "        if is_training == True:\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        data_loop.set_description(f'Epoch {ecpoch}')\n",
    "        data_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    mse = mean_squared_error(ground_truth_s, prediction_s)\n",
    "    rmse = sqrt(mse)\n",
    "    mae = mean_absolute_error(ground_truth_s, prediction_s)\n",
    "    r2 = r2_score(ground_truth_s, prediction_s)\n",
    "    \n",
    "\n",
    "    metrices = {'MSE':mse ,\n",
    "                      'RMSE':rmse, \n",
    "                      'MAE':mae, \n",
    "                      'R²':r2,\n",
    "                      'loss': (sum(losses) / len(losses))}\n",
    "    return metrices , prediction_s, ground_truth_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd188e0-a6f3-4f5f-b9a6-5a9040dd42e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  41%|████▏     | 104/251 [03:00<04:21,  1.78s/it, loss=0.0128] "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "global_metrices = []\n",
    "v_global_metrices = []\n",
    "\n",
    "\n",
    "for ecpoch in range(EPOCHS):\n",
    "    \n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    model.train()\n",
    "    metrices,prediction_s, ground_truth_s  = training_loop(model ,train_loop, is_training = True)\n",
    "    global_metrices.append(metrices)\n",
    "    print(\"Training metrices \",metrices)\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        validation_loop = tqdm(validation_loader, leave=True)\n",
    "        v_metrices, v_prediction_s, v_ground_truth_s  = training_loop(model ,validation_loop, is_training = False)\n",
    "\n",
    "\n",
    "        demo_len =100000\n",
    "        for i in range(int(minimum(demo_len , len(v_prediction_s) ))):\n",
    "\n",
    "            print('\\n')\n",
    "            print( v_prediction_s[i], v_ground_truth_s[i] )\n",
    "            print(tokenizer.decode(validation_dataset[i][0].input_ids[0],skip_special_tokens=True).split('[SEP]')[0] )\n",
    "\n",
    "           \n",
    "        print( 'v_metrices: ',v_metrices )\n",
    "        v_global_metrices.append(v_metrices)\n",
    "        \n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(MODEL_SAVE_PATH + EXPERIMENT_NAME)\n",
    "\n",
    "        print('SAVING MODEL @ ',MODEL_SAVE_PATH +EXPERIMENT_NAME)\n",
    "        unwrapped_model.save_pretrained(MODEL_SAVE_PATH +EXPERIMENT_NAME)\n",
    "        print('saved')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c17257-46c0-47e1-8906-11b9ecfe0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to script data_pipe.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7802808-ad53-44ec-9468-aff785cfa0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
