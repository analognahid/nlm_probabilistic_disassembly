{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d3e234-741d-493b-89f7-f00b39b6ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "# from accelerate import notebook_launcher\n",
    "\n",
    "# Optionally, you can specify the number of processes like this:\n",
    "accelerator.state.num_processes = 3  # For 3 GPUs\n",
    "\n",
    "# This will automatically handle device placement for you\n",
    "device = accelerator.device\n",
    "\n",
    "\n",
    "import torch\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a994e3c-4bb4-4196-840d-3c78c182596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys, json\n",
    "import magic, hashlib, os, traceback\n",
    "import ntpath\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import traceback\n",
    "import lief\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from transformers import AdamW,AutoTokenizer\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score,f1_score, confusion_matrix,mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f654b05f-145b-405f-b917-6bd69545e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_path = '/home/raisul/DATA/x86_O2_d4/' #'/home/raisul/DATA/x86_O2_d4_mingw32_PE' \n",
    "\n",
    "bin_files = [os.path.join(bin_path, f) for f in os.listdir(bin_path) ][0:2000]\n",
    "\n",
    "ground_truth_path ='/home/raisul/ANALYSED_DATA/ghidra_x86_O2_d4/' # '/home/raisul/ANALYSED_DATA/ghidra_x86_O2_d4_mingw32_PE'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0dde3c-536e-45b0-8d59-26706335e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_SIZE = 256\n",
    "BATCH_SIZE = 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd0acbc1-ce7c-4468-a899-46b9641d8ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bin_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d01dfce-edb1-4a0a-a0a8-fbaf3fac12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ground_truth_ghidra(exe_path, text_section_offset , text_section_len):\n",
    "\n",
    "    text_sextion_end = text_section_offset + text_section_len\n",
    "    \n",
    "    elf_file_name = os.path.basename(exe_path)\n",
    "    ghidra_file_path = os.path.join(ground_truth_path, elf_file_name.split('.')[0]) + '.json'\n",
    "    \n",
    "    with open(ghidra_file_path, \"r\") as file:\n",
    "        ghidra_data = json.load(file)\n",
    "\n",
    "    ground_truth_offsets = list(ghidra_data.keys())\n",
    "\n",
    "    ground_truth_offsets = [int(i) for i in ground_truth_offsets]\n",
    "    ground_truth_offsets = [x for x in ground_truth_offsets if text_section_offset <= x <= text_sextion_end]\n",
    "    ground_truth_offsets.sort()\n",
    "    return ground_truth_offsets\n",
    "\n",
    "\n",
    "\n",
    "def find_data_in_textsection(ground_truth_offsets , text_section_offset , text_section_len, offset_inst_dict):\n",
    "    data_offsets = []\n",
    "    for i in range(1, len(ground_truth_offsets)-1):\n",
    "        distance = ground_truth_offsets[i+1] - ground_truth_offsets[i]\n",
    "\n",
    "        inst_len = offset_inst_dict[ground_truth_offsets[i]].size \n",
    "        \n",
    "        if distance!=inst_len:\n",
    "            # print('offset_ranges[i]: ',ground_truth_offsets[i] , 'offset_ranges[i-1]: ',ground_truth_offsets[i-1], ' inst_len: ',inst_len  )\n",
    "            \n",
    "            # print(ground_truth_offsets[i],' ' ,hex(ground_truth_offsets[i]) , offset_inst_dict[ground_truth_offsets[i]], ' len',offset_inst_dict[ground_truth_offsets[i]].size )\n",
    "            # print(\"\\nByte GAP ###### \",distance ,' Missing bytes: ', distance - inst_len)\n",
    "            \n",
    "            for j in range( ground_truth_offsets[i] +inst_len , ground_truth_offsets[i+1]  ):\n",
    "                data_offsets.append(j)\n",
    "                # if offset_inst_dict[j]:\n",
    "                #     print(\"# \",j, offset_inst_dict[j].mnemonic, offset_inst_dict[j].op_str , 'inst len:',offset_inst_dict[j].size )\n",
    "                # else:\n",
    "                #     print(\"# \",j, \" invalid \")\n",
    "            # print('\\n')\n",
    "        else:\n",
    "            # print(ground_truth_offsets[i],' ', hex(ground_truth_offsets[i]) , offset_inst_dict[ground_truth_offsets[i]].mnemonic,offset_inst_dict[ground_truth_offsets[i]].op_str ,' len',offset_inst_dict[ground_truth_offsets[i]].size)\n",
    "            pass\n",
    "    return data_offsets\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2f8ec7-72bd-4fd8-aef3-90ef4e0fcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linear_sweep(offset_inst , target_offset):\n",
    "    inst_sequence = ''\n",
    "    max_seq_length = 15\n",
    "\n",
    "    \n",
    "    current_offset = target_offset\n",
    "    for q in range(max_seq_length):\n",
    "\n",
    "        if current_offset in offset_inst: #if end of text section\n",
    "            current_instruction = offset_inst[current_offset]\n",
    "            if current_instruction is None:\n",
    "\n",
    "\n",
    "                return  None\n",
    "                \n",
    "            current_offset = current_offset + current_instruction.size\n",
    "            inst_sequence+= str( hex(current_instruction.address)) +\" \"+ current_instruction.mnemonic +' '+ current_instruction.op_str+ ' ; ' \n",
    "\n",
    "            if current_instruction.mnemonic in [\"ret\", \"jmp\"]: #break linear sweep\n",
    "                return inst_sequence\n",
    "\n",
    "    return inst_sequence\n",
    "\n",
    "\n",
    "SEQUENCES = []\n",
    "LABELS     = []\n",
    "\n",
    "for bin_file_path in bin_files:\n",
    "\n",
    "    \n",
    "    md = Cs(CS_ARCH_X86, CS_MODE_64)\n",
    "    md.detail = True\n",
    "    offset_inst = {}\n",
    "\n",
    "    \n",
    "    with open(bin_file_path, 'rb') as f:\n",
    "\n",
    "        try:\n",
    "            elffile = ELFFile(f)\n",
    "           \n",
    "            textSection = elffile.get_section_by_name('.text').data()\n",
    "        \n",
    "            text_section_offset = elffile.get_section_by_name('.text')['sh_offset']\n",
    "          \n",
    "            ground_truth_offsets = get_ground_truth_ghidra(bin_file_path, text_section_offset , len(textSection))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e ,bin_file_path)\n",
    "            continue\n",
    "\n",
    "    for byte_index in range(len(textSection)):\n",
    "        \n",
    "    \n",
    "        try:    \n",
    "\n",
    "            instruction = next(md.disasm(textSection[byte_index: byte_index+15 ], text_section_offset + byte_index ), None)\n",
    "            offset_inst[text_section_offset+byte_index] = instruction\n",
    "            \n",
    "            # if instruction:\n",
    "            #     print(\"%d:\\t%s\\t%s _\\t%x\" %(int(instruction.address), instruction.mnemonic, instruction.op_str, instruction.size))\n",
    "            # else:\n",
    "            #     print(\"%d:\\t%s \" % (text_section_offset + byte_index  , 'invalid instruction') )\n",
    "                \n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(traceback.print_exc() )\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "    \n",
    "    offset_inst_dict = collections.OrderedDict(sorted(offset_inst.items()))\n",
    "\n",
    "    DATA_OFFSETS = find_data_in_textsection(ground_truth_offsets , text_section_offset , len(textSection) , offset_inst)\n",
    "\n",
    "\n",
    "    \n",
    "    for byte_offset in range(text_section_offset, text_section_offset+len(textSection)):\n",
    "        inst_seq = linear_sweep(offset_inst_dict ,  byte_offset )\n",
    "        if inst_seq== None:\n",
    "            continue\n",
    "\n",
    "        SEQUENCES.append(inst_seq)\n",
    "        if byte_offset in ground_truth_offsets:\n",
    "            LABELS.append(float(0))\n",
    "        else:\n",
    "            LABELS.append(float(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7fa81ca-66f4-4bd8-ad3e-aac8e4ea7d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2975 2975\n",
      "909 2066\n"
     ]
    }
   ],
   "source": [
    "print(len(SEQUENCES) , len(LABELS))\n",
    "print(LABELS.count(0), LABELS.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617b62c7-6e25-49ea-beb7-3796d52d44ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x1060 endbr64  ; 0x1064 push rbx ; 0x1065 lea rbx, [rip + 0xf98] ; 0x106c mov edx, 0x68 ; 0x1071 xor eax, eax ; 0x1073 mov rsi, rbx ; 0x1076 mov edi, 2 ; 0x107b call 0x1050 ; 0x1080 mov rsi, rbx ; 0x1083 mov edx, 0x78 ; 0x1088 xor eax, eax ; 0x108a mov edi, 2 ; 0x108f call 0x1050 ; 0x1094 xor eax, eax ; 0x1096 pop rbx ; '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQUENCES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0307133-f04b-43dc-b48f-1a71d755ce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "from transformers import BertTokenizer,BertForSequenceClassification\n",
    "\n",
    "# If using a character-level tokenizer for sequences like DNA/Protein:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")#BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = tokenizer.train_new_from_iterator(SEQUENCES, 2000)\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=1  \n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optim = AdamW( model.parameters() , lr=1e-5, eps = 1e-6, betas=(0.9,0.98), weight_decay=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19a59b-6531-4b35-9ce4-5bfc7afb299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b4129cd-e356-4327-9e23-72beed13b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Tokenize the \n",
    "        tokenized_text = (self.tokenizer(text , max_length= MAX_TOKEN_SIZE,padding='max_length', truncation=True , return_tensors='pt')).to(device)\n",
    "        \n",
    "        # Convert tokens to input IDs\n",
    "#         input_ids = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        \n",
    "        # Create input tensors\n",
    "#         input_ids = tokenized_text['input_ids']  #torch.tensor(input_ids)\n",
    "        # label = torch.tensor([label]).to(device)\n",
    "        return tokenized_text, label\n",
    "        \n",
    "#         return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fa283cd-ae7b-4b14-aef8-d197ca0ad338",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BinaryDataset(SEQUENCES, LABELS,tokenizer)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "validation_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(dataset, [train_size, validation_size] , generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0759f95b-3573-42f9-b4ff-f7ab891492c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2975\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c119b9-3fb7-4264-be17-e50d77f81283",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE ,shuffle=True) \n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f3296ff-458f-48d5-be35-85e6dcd7a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optim, train_loader,validation_loader = accelerator.prepare(model, optim, train_loader,validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fff2a619-6a06-4774-8ec7-bad9a55e8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def training_loop(model ,data_loop, is_training = False):\n",
    "    \n",
    "    prediction_s, ground_truth_s = [], []\n",
    "    losses = []\n",
    "\n",
    "    for N,batch in enumerate(data_loop):\n",
    "        \n",
    "\n",
    "        # Forward pass\n",
    "        if is_training == True:\n",
    "            optim.zero_grad()\n",
    "        \n",
    "        batch_input, batch_labels = batch\n",
    "        if len(batch_labels)<BATCH_SIZE:\n",
    "            continue\n",
    "            \n",
    "        batch_input_ids= batch_input['input_ids']\n",
    "        batch_attention_mask=batch_input['attention_mask']\n",
    "        batch_token_type_ids =batch_input['token_type_ids']\n",
    "        \n",
    "        outputs = model(input_ids=batch_input_ids.squeeze(),\n",
    "                        attention_mask=batch_attention_mask.squeeze(),\n",
    "                        token_type_ids=batch_token_type_ids.squeeze(),\n",
    "                        labels=batch_labels.float() )\n",
    "        \n",
    "#\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # print(logits)\n",
    "\n",
    "        # print(logits.squeeze())\n",
    "\n",
    "        prediction_s.extend(predictions.detach().cpu().numpy().flatten())\n",
    "        ground_truth_s.extend(batch_labels.detach().cpu().numpy().flatten())\n",
    "\n",
    "\n",
    "        if is_training == True:\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        data_loop.set_description(f'Epoch {ecpoch}')\n",
    "        data_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    ###### Training Scores\n",
    "    # accuracy = accuracy_score(ground_truth_s, prediction_s)    \n",
    "    # precision, recall, f1, _ = precision_recall_fscore_support(ground_truth_s,prediction_s,average='weighted')\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    mse = mean_squared_error(ground_truth_s, prediction_s)\n",
    "    rmse = sqrt(mse)\n",
    "    mae = mean_absolute_error(ground_truth_s, prediction_s)\n",
    "    r2 = r2_score(ground_truth_s, prediction_s)\n",
    "    \n",
    "    # print(f\"MSE: {mse:.4f}\")\n",
    "    # print(f\"RMSE: {rmse:.4f}\")\n",
    "    # print(f\"MAE: {mae:.4f}\")\n",
    "    # print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "\n",
    "    metrices = {'MSE':mse ,\n",
    "                      'RMSE':rmse, \n",
    "                      'MAE':mae, \n",
    "                      'R²':r2,\n",
    "                      'loss': (sum(losses) / len(losses))}\n",
    "    return metrices , prediction_s, ground_truth_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fd188e0-a6f3-4f5f-b9a6-5a9040dd42e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.62s/it, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrices  {'MSE': np.float64(0.7026666666666667), 'RMSE': np.float64(0.8382521498133283), 'MAE': np.float64(0.7026666666666667), 'R²': -2.3632286995515694, 'loss': np.float64(0.21901147895389134)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.93it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_metrices:  {'MSE': np.float64(0.64), 'RMSE': np.float64(0.8), 'MAE': np.float64(0.64), 'R²': -1.7777777777777781, 'loss': np.float64(0.22029319405555725)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.65s/it, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrices  {'MSE': np.float64(0.712), 'RMSE': np.float64(0.8438009243891594), 'MAE': np.float64(0.712), 'R²': -2.4722222222222223, 'loss': np.float64(0.2154316852490107)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.88it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_metrices:  {'MSE': np.float64(0.674), 'RMSE': np.float64(0.8209750300709517), 'MAE': np.float64(0.674), 'R²': -2.067484662576687, 'loss': np.float64(0.21226752549409866)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.69s/it, loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrices  {'MSE': np.float64(0.7062222222222222), 'RMSE': np.float64(0.8403702887550357), 'MAE': np.float64(0.7062222222222222), 'R²': -2.4039334341906207, 'loss': np.float64(0.20774587161011165)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.85it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_metrices:  {'MSE': np.float64(0.636), 'RMSE': np.float64(0.7974960814950754), 'MAE': np.float64(0.636), 'R²': -1.7472527472527468, 'loss': np.float64(0.21964935213327408)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:17<00:00,  1.72s/it, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrices  {'MSE': np.float64(0.7075555555555556), 'RMSE': np.float64(0.8411632157646669), 'MAE': np.float64(0.7075555555555556), 'R²': -2.4194528875379944, 'loss': np.float64(0.2021484375)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.85it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_metrices:  {'MSE': np.float64(0.638), 'RMSE': np.float64(0.7987490219086343), 'MAE': np.float64(0.638), 'R²': -1.762430939226519, 'loss': np.float64(0.21454650908708572)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:17<00:00,  1.73s/it, loss=0.177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrices  {'MSE': np.float64(0.7106666666666667), 'RMSE': np.float64(0.8430104783848578), 'MAE': np.float64(0.7106666666666667), 'R²': -2.456221198156682, 'loss': np.float64(0.19779066410329607)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.84it/s, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_metrices:  {'MSE': np.float64(0.644), 'RMSE': np.float64(0.8024961059095552), 'MAE': np.float64(0.644), 'R²': -1.8089887640449436, 'loss': np.float64(0.20684447139501572)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:17<00:00,  1.74s/it, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrices  {'MSE': np.float64(0.7044444444444444), 'RMSE': np.float64(0.8393118874676114), 'MAE': np.float64(0.7044444444444444), 'R²': -2.3834586466165417, 'loss': np.float64(0.19211516280968985)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.81it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_metrices:  {'MSE': np.float64(0.65), 'RMSE': np.float64(0.806225774829855), 'MAE': np.float64(0.65), 'R²': -1.8571428571428572, 'loss': np.float64(0.19881979376077652)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:17<00:00,  1.74s/it, loss=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrices  {'MSE': np.float64(0.7031111111111111), 'RMSE': np.float64(0.8385172097882733), 'MAE': np.float64(0.7031111111111111), 'R²': -2.3682634730538923, 'loss': np.float64(0.1915250387456682)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     18\u001b[0m validation_loop \u001b[38;5;241m=\u001b[39m tqdm(validation_loader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m v_metrices, v_prediction_s, v_ground_truth_s  \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_loop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_metrices: \u001b[39m\u001b[38;5;124m'\u001b[39m,v_metrices)\n\u001b[1;32m     21\u001b[0m v_global_metrices\u001b[38;5;241m.\u001b[39mappend(v_metrices)\n",
      "Cell \u001b[0;32mIn[16], line 29\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, data_loop, is_training)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 29\u001b[0m         losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m         logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     32\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "global_metrices = []\n",
    "v_global_metrices = []\n",
    "\n",
    "\n",
    "for ecpoch in range(EPOCHS):\n",
    "    \n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    model.train()\n",
    "    metrices,prediction_s, ground_truth_s  = training_loop(model ,train_loop, is_training = True)\n",
    "    global_metrices.append(metrices)\n",
    "    print(\"Training metrices \",metrices)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        validation_loop = tqdm(validation_loader, leave=True)\n",
    "        v_metrices, v_prediction_s, v_ground_truth_s  = training_loop(model ,validation_loop, is_training = False)\n",
    "        print('v_metrices: ',v_metrices)\n",
    "        v_global_metrices.append(v_metrices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c17257-46c0-47e1-8906-11b9ecfe0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to script data_pipe.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed5162-0a87-447c-abde-75908f70cafa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
