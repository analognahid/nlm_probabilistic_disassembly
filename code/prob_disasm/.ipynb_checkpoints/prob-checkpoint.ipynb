{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a994e3c-4bb4-4196-840d-3c78c182596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys, json,re, pickle\n",
    "import magic, hashlib,  traceback ,ntpath, collections ,lief\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "import torch.nn as nn\n",
    "import lief\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from transformers import AdamW,AutoTokenizer\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score,f1_score, confusion_matrix,mean_squared_error, mean_absolute_error, r2_score\n",
    "from numpy import *\n",
    "from num2words import num2words\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fa61a1-db3e-46d9-b082-4ac6bcf0105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 1000  # Prevent infinite looping\n",
    "NEAR_JUMP = 0.00001525902  # (2^32 - 1)^-1\n",
    "REL_JUMP = 0.00392156862  # (2^16 - 1)^-1\n",
    "JUST_JUMP = 0.00390625 #todo fix prob\n",
    "DEF_USE = 1/16\n",
    "BOTTOM = None\n",
    "\n",
    "CONTROL_GROUPS = {\n",
    "    CS_GRP_JUMP,\n",
    "    CS_GRP_CALL,\n",
    "    CS_GRP_RET,\n",
    "    CS_GRP_IRET,\n",
    "}\n",
    "BRANCH_GROUPS = {\n",
    "    CS_GRP_CALL,       # Function call instruction\n",
    "    CS_GRP_JUMP       # Conditional and unconditional branches\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f654b05f-145b-405f-b917-6bd69545e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_FILE_TYPE = 'PE' #or ELF\n",
    "bin_path = '/home/raisul/DATA/temp/x86_pe_msvc_O2_static/'\n",
    "bin_files = [os.path.join(bin_path, f) for f in os.listdir(bin_path) if f.endswith(\".exe\")]\n",
    "ground_truth_path ='/home/raisul/DATA/temp/ghidra_x86_pe_msvc_O2_debug/'  \n",
    "save_path = '/home/raisul/ANALYSED_DATA/prob_disasm_pe/'\n",
    "\n",
    "# BIN_FILE_TYPE = 'ELF'\n",
    "# bin_path = '/home/raisul/DATA/x86_O2_d4/' #/home/raisul/DATA/temp/x86_pe_msvc_O2_static/'\n",
    "# bin_files = [f for f in os.listdir(bin_path) ]\n",
    "# ground_truth_path ='/home/raisul/ANALYSED_DATA/ghidra_x86_O2_d4/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d01dfce-edb1-4a0a-a0a8-fbaf3fac12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ground_truth_ghidra(exe_path, text_section_offset , text_section_len):\n",
    "\n",
    "    text_sextion_end = text_section_offset + text_section_len\n",
    "    \n",
    "    elf_file_name = os.path.basename(exe_path)\n",
    "    ghidra_file_path = os.path.join(ground_truth_path, elf_file_name.split('.')[0]) + '.json'\n",
    "    \n",
    "    with open(ghidra_file_path, \"r\") as file:\n",
    "        ghidra_data = json.load(file)\n",
    "\n",
    "    ground_truth_offsets = list(ghidra_data.keys())\n",
    "\n",
    "    ground_truth_offsets = [int(i) for i in ground_truth_offsets]\n",
    "    ground_truth_offsets = [x for x in ground_truth_offsets if text_section_offset <= x <= text_sextion_end]\n",
    "    ground_truth_offsets.sort()\n",
    "    return ground_truth_offsets\n",
    "\n",
    "\n",
    "\n",
    "def find_data_in_textsection(ground_truth_offsets , text_section_offset , text_section_len, offset_inst_dict):\n",
    "    data_offsets = []\n",
    "    for i in range(1, len(ground_truth_offsets)-1):\n",
    "        distance = ground_truth_offsets[i+1] - ground_truth_offsets[i]\n",
    "\n",
    "        inst_len = offset_inst_dict[ground_truth_offsets[i]].size \n",
    "        \n",
    "        if distance!=inst_len:\n",
    "            # print('offset_ranges[i]: ',ground_truth_offsets[i] , 'offset_ranges[i-1]: ',ground_truth_offsets[i-1], ' inst_len: ',inst_len  )\n",
    "            # print(ground_truth_offsets[i],' ' ,hex(ground_truth_offsets[i]) , offset_inst_dict[ground_truth_offsets[i]], ' len',offset_inst_dict[ground_truth_offsets[i]].size )\n",
    "            # print(\"\\nByte GAP ###### \",distance ,' Missing bytes: ', distance - inst_len)\n",
    "            \n",
    "            for j in range( ground_truth_offsets[i] +inst_len , ground_truth_offsets[i+1]  ):\n",
    "                data_offsets.append(j)\n",
    "                # if offset_inst_dict[j]:\n",
    "                #     print(\"# \",j, offset_inst_dict[j].mnemonic, offset_inst_dict[j].op_str , 'inst len:',offset_inst_dict[j].size )\n",
    "                # else:\n",
    "                #     print(\"# \",j, \" invalid \")\n",
    "            # print('\\n')\n",
    "        else:\n",
    "            # print(ground_truth_offsets[i],' ', hex(ground_truth_offsets[i]) , offset_inst_dict[ground_truth_offsets[i]].mnemonic,offset_inst_dict[ground_truth_offsets[i]].op_str ,' len',offset_inst_dict[ground_truth_offsets[i]].size)\n",
    "            pass\n",
    "    return data_offsets\n",
    "    \n",
    "\n",
    "def linear_sweep(offset_inst , target_offset):\n",
    "    inst_sequence = ''\n",
    "    address_list = []\n",
    "    \n",
    "    current_offset = target_offset\n",
    "    for q in range(MAX_SEQUENCE_LENGTH):\n",
    "\n",
    "        if current_offset in offset_inst: #if end of text section\n",
    "            current_instruction = offset_inst[current_offset]\n",
    "            if current_instruction is None:\n",
    "                return  None\n",
    "                \n",
    "            current_offset = current_offset + current_instruction.size\n",
    "            inst_sequence+= str( hex(current_instruction.address)) +\" \"+ current_instruction.mnemonic +' '+ current_instruction.op_str+ ' ; ' \n",
    "            address_list.append(current_instruction.address)\n",
    "            \n",
    "            if current_instruction.mnemonic in [\"ret\", \"jmp\"]: #break linear sweep\n",
    "                break\n",
    "                \n",
    "\n",
    "    return inst_sequence, address_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c17257-46c0-47e1-8906-11b9ecfe0be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _compute_destinations(disasm):\n",
    "    \"\"\" Compute successor addresses (CFG) and ensure function epilogues are correctly identified. \"\"\"\n",
    "    dests, preds = {}, defaultdict(list)\n",
    "    last_offset = list(disasm.keys())[-1]\n",
    "    first_offset = list(disasm.keys())[0]\n",
    "\n",
    "    for offset, details in disasm.items():\n",
    "        if details==None:\n",
    "            continue\n",
    "        inst_str = details.mnemonic +' ' + details.op_str\n",
    "        next_offset = offset + details.size\n",
    "\n",
    "\n",
    "\n",
    "        if not set(details.groups) & CONTROL_GROUPS:\n",
    "            # Default fallthrough for non-control flow instructions\n",
    "            if next_offset <= last_offset:\n",
    "                dests[offset] = [next_offset]\n",
    "                # preds[next_offset].append(offset)\n",
    "            else:\n",
    "                dests[offset] = []\n",
    "        else: #control instruction\n",
    "            #unconditional jump\n",
    "            if details.id == X86_INS_JMP and details.operands and details.operands[0].type == CS_OP_IMM:\n",
    "                 # Unconditional jump\n",
    "                op_value = details.operands[0].imm\n",
    "                if op_value>=first_offset and op_value<=last_offset:\n",
    "                    dests[offset] = [op_value]\n",
    "                    # preds[op_value].append(offset)\n",
    "            \n",
    "            # elif \"COND_BR\" in details.groups or \"CALL\" in details.groups:\n",
    "            elif (CS_GRP_JUMP in details.groups or CS_GRP_CALL in details.groups) :\n",
    "                if details.operands and details.operands[0].type == CS_OP_IMM:\n",
    "                    jump_target = details.operands[0].imm\n",
    "    \n",
    "                    if next_offset<=last_offset:\n",
    "                        dests[offset] = [next_offset]\n",
    "                        # preds[next_offset].append(offset)\n",
    "                                     \n",
    "                    if jump_target>=first_offset and jump_target<=last_offset and jump_target!=next_offset:\n",
    "                        if offset in dests:\n",
    "                            dests[offset].append(jump_target)\n",
    "                        else:\n",
    "                            dests[offset] = [jump_target]\n",
    "        \n",
    "            else:\n",
    "                # print('>>>>>  ',offset, ' : ' ,inst_str)\n",
    "                dests[offset] = None\n",
    "\n",
    "        if offset in dests:\n",
    "            if dests[offset] is not None:\n",
    "                for target in dests[offset]:\n",
    "                    preds[target].append(offset)\n",
    "\n",
    "    return dests, preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7fd80f7-1677-4845-a544-a3fabe50ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _compute_occlusion(disasm):\n",
    "    \"\"\" Identify overlapping instructions and remove \"\"\"\n",
    "    occlusion = defaultdict(list)\n",
    "    valid_instructions = set()\n",
    "\n",
    "    for offset, details in disasm.items():\n",
    "        if details!= None:\n",
    "            for i in range(offset + 1, offset + details.size):\n",
    "                occlusion[i].append(offset)\n",
    "\n",
    "    # fix nahid\n",
    "    covered = set()\n",
    "    for offset in sorted(disasm.keys()):\n",
    "        if disasm[offset] is None:\n",
    "            continue\n",
    "        if offset in covered:\n",
    "            # print(f\"Skipping {offset} due to occlusion\")\n",
    "            continue  # Skip if another instruction already claimed this byte\n",
    "\n",
    "        valid_instructions.add(offset)\n",
    "        for i in range(offset, offset + disasm[offset].size):\n",
    "            covered.add(i)  # Mark all bytes of this instruction as covered\n",
    "\n",
    "    # print(f\"Final valid instructions after occlusion: {sorted(valid_instructions)}\")\n",
    "    return occlusion, valid_instructions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d25e8e-6afe-4c84-9479-706c117738e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "def get_recursive_descent_cfg(disasm,ALL_RD_CFG):\n",
    "    debug = False\n",
    "    # global ALL_RD_CFG, cfg\n",
    "\n",
    "    for offset in disasm:\n",
    "        if debug:\n",
    "            print('\\n-------------------------------------------------------\\n')\n",
    "        RD_CFG = []\n",
    "        current = offset\n",
    "        while True:\n",
    "\n",
    "            # print('current: ',current)\n",
    "\n",
    "            #prevent cycle\n",
    "            if current in RD_CFG:\n",
    "                break\n",
    "            #out of bound\n",
    "            if current not in disasm:\n",
    "                break\n",
    "            #current invalid\n",
    "            if disasm[current] is None:\n",
    "                break\n",
    "\n",
    "\n",
    "            #return\n",
    "            if disasm[current].mnemonic in [\"ret\"]:\n",
    "                break\n",
    "            \n",
    "\n",
    "            if disasm[current].mnemonic == 'jmp':\n",
    "                if disasm[current].operands[0].type == CS_OP_IMM:\n",
    "                    next =  disasm[current].operands[0].imm\n",
    "                else:\n",
    "                    #todo fix indirect\n",
    "                    if debug:\n",
    "                        print(offset, disasm[offset])\n",
    "                    break\n",
    "            else :\n",
    "                next =  current + disasm[current].size \n",
    "\n",
    "\n",
    "            if next not in disasm:\n",
    "                break\n",
    "            RD_CFG.append(next)\n",
    "            current = next \n",
    "            \n",
    "        #save         \n",
    "        ALL_RD_CFG [offset] = RD_CFG\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07dba9af-430a-4c63-98e5-e9906bc7d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_recursive_preds(disasm ,ALL_RD_PRED ,ALL_RD_CFG):\n",
    "\n",
    "    #todo fix nahid\n",
    "    for offset in disasm:\n",
    "        ALL_RD_PRED[offset] = []\n",
    "\n",
    "    for offset in disasm:\n",
    "        for target in ALL_RD_CFG[offset] :\n",
    "            if target in disasm: #last byte\n",
    "                if target not in ALL_RD_PRED[target] :\n",
    "                    ALL_RD_PRED[target].append(offset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa78b45-c59f-40de-b459-cba93fab3bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184032c-4df0-4f26-940f-f93658b9ccf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d902dcc-0da1-46a1-a0ee-8905bd3ad4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize(disasm,data_prob,H_list):\n",
    "    for offset in range(list(disasm.keys())[0], list(disasm.keys())[-1]  + 1):\n",
    "    \n",
    "            if disasm[offset] is  None:\n",
    "                data_prob[offset] =  1.0\n",
    "            # elif offset in valid_instructions:\n",
    "            #     data_prob[offset] =  0.9\n",
    "            else:\n",
    "                data_prob[offset] = BOTTOM\n",
    "    \n",
    "            H_list[offset] = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf5d163-63b7-46d4-ac66-34b7553ddd7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _hint_one(offset, disasm, preds, H_list):\n",
    "\n",
    "    \"\"\" Implements Control Flow Convergence hint. \"\"\"\n",
    "    debug = False\n",
    "\n",
    "    if offset not in preds:\n",
    "        return\n",
    "\n",
    "    branches = [prev for prev in preds[offset] if set(disasm[prev].groups) & BRANCH_GROUPS]\n",
    "\n",
    "    if disasm[offset]:\n",
    "        if debug:\n",
    "            print( '#  ' if offset in ground_truth_offsets else '   ', \\\n",
    "                  ' $ ' if len(set(disasm[offset].groups) & BRANCH_GROUPS) else '   ', \\\n",
    "                  ' ',offset , ' : ', hex(offset),' ',disasm[offset].mnemonic +' ' + disasm[offset].op_str  , ' ' , disasm[offset].size)\n",
    "\n",
    "    if len(branches)<2:\n",
    "        return\n",
    "\n",
    "    if debug:\n",
    "        print(branches)\n",
    "    for branch in branches:\n",
    "        # H[branch].add((\"1rel\" if disasm[branch].size == 2 else \"1near\", offset))\n",
    "\n",
    "        jump_len = disasm[branch].operands[0].imm - (disasm[branch].address + disasm[branch].size)\n",
    "        \n",
    "        if -128 <= jump_len <= 127:\n",
    "            H_list[branch].append( REL_JUMP )\n",
    "            H_list[offset].append( REL_JUMP) \n",
    "        else:\n",
    "            H_list[branch].append( NEAR_JUMP)\n",
    "            H_list[offset].append( NEAR_JUMP )\n",
    "        if debug:\n",
    "            print('$$$ len', jump_len)\n",
    "        \n",
    "\n",
    "def _hint_one_one(offset, disasm, cfg, H_list):\n",
    "\n",
    "    \"\"\" Implements valid jump. \"\"\"\n",
    "    debug = False\n",
    "\n",
    "\n",
    "\n",
    "    if set(disasm[offset].groups) & BRANCH_GROUPS:\n",
    "        next_offset_by_size = offset + disasm[offset].size\n",
    "        if offset in cfg:\n",
    "            if any(element != next_offset_by_size for element in cfg[offset]):\n",
    "                H_list[offset].append( JUST_JUMP )\n",
    "\n",
    "        \n",
    "\n",
    "def _hint_two(offset, disasm,preds,H_list):\n",
    "\n",
    "    \"\"\" Implements Control Flow Crossing hint. \"\"\"\n",
    "    debug = False\n",
    "    \n",
    "    if debug:\n",
    "        # print( '#' if offset in ground_truth_offsets else ' ',' ',offset , ' : ', hex(offset),' ',disasm[offset].mnemonic +' ' + disasm[offset].op_str  , ' ' , disasm[offset].size)\n",
    "        print( '#  ' if offset in ground_truth_offsets else '   ', \\\n",
    "              ' $ ' if len(set(disasm[offset].groups) & BRANCH_GROUPS) else '   ', \\\n",
    "              ' ',offset , ' : ', hex(offset),' ',disasm[offset].mnemonic +' ' + disasm[offset].op_str  , ' ' , disasm[offset].size)\n",
    "    inst2_offset = offset\n",
    "    if not CS_GRP_JUMP in disasm[inst2_offset].groups: #inst2 have to be a control one\n",
    "        return\n",
    "    \n",
    "    \n",
    "    inst2_size   = disasm[inst2_offset].size\n",
    "    inst3_offset =  inst2_offset + inst2_size\n",
    "\n",
    "    if inst3_offset not in preds: #inst 3 has to be a target of inst1\n",
    "        return\n",
    "\n",
    "    inst3_preds_list = preds[inst3_offset]\n",
    "\n",
    "    if inst2_offset in inst3_preds_list:\n",
    "        inst3_preds_list.remove(inst2_offset)\n",
    "\n",
    "    for inst1_offset in inst3_preds_list:\n",
    "        if CS_GRP_JUMP in disasm[inst1_offset].groups: #ins1 has to be a control flow instruction\n",
    "\n",
    "            inst1_jump_len = disasm[inst1_offset].operands[0].imm - (disasm[inst1_offset].address + disasm[inst1_offset].size)\n",
    "            inst2_jump_len = disasm[inst2_offset].operands[0].imm - (disasm[inst2_offset].address + disasm[inst2_offset].size)\n",
    "\n",
    "            \n",
    "\n",
    "            if -128 <= inst1_jump_len <= 127:  \n",
    "                hint_type_inst1 = NEAR_JUMP\n",
    "            else:\n",
    "                hint_type_inst1 = REL_JUMP\n",
    "\n",
    "            if -128 <= inst2_jump_len <= 127:  \n",
    "                hint_type_inst2 = NEAR_JUMP\n",
    "            else:\n",
    "                hint_type_inst2 = REL_JUMP\n",
    "\n",
    "            H_list[inst1_offset].append( hint_type_inst1 )\n",
    "            H_list[inst3_offset].append( hint_type_inst1 )\n",
    "            H_list[inst2_offset].append( hint_type_inst2 )\n",
    "\n",
    "\n",
    "            if debug:\n",
    "                print('inst1: ', inst1_offset , 'inst2: ',inst2_offset, 'inst3: ',inst3_offset)\n",
    "                for o in [inst1_offset , inst2_offset, inst3_offset]:\n",
    "                    if o not in ground_truth_offsets:\n",
    "                            print('# # # '*10 ,o)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _hint_three(offset, disasm, preds,H_list):\n",
    "\n",
    "    \"\"\" Implements Register Define-Use Relation hint. \"\"\"\n",
    "\n",
    "    debug = False\n",
    "    # if offset not in ground_truth_offsets:\n",
    "    #     return\n",
    "\n",
    "    \n",
    "    if debug:\n",
    "        print('\\n\\n')\n",
    "        print( '#' if offset in ground_truth_offsets else ' ',' ',offset , ' : ', hex(offset),' ',disasm[offset].mnemonic +' ' + disasm[offset].op_str  , ' ' , disasm[offset].size)\n",
    "\n",
    "        regs_read, regs_write = disasm[offset].regs_access()\n",
    "        current_reads = set(disasm[offset].reg_name(r) for r in regs_read)\n",
    "        current_writes = set(disasm[offset].reg_name(r) for r in regs_write)\n",
    "        \n",
    "        print('read'    , current_reads   )\n",
    "        print('writes'  , current_writes  )\n",
    "        \n",
    "        # print('           current read: ',disasm[offset].regs_read , \\\n",
    "        #       [disasm[offset].reg_name(r) for r in disasm[offset].regs_read ] ,  \\\n",
    "        #       [md.reg_name(r) for r in disasm[offset].regs_read ]) #if md.reg_name(r) not in ('eflags', 'rflags')]\n",
    "        # print('           current WRITE: ',  [disasm[offset].reg_name(r) for r in disasm[offset].regs_write] ) #if md.reg_name(r) not in ('eflags', 'rflags')]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if offset not in preds:\n",
    "        return\n",
    "\n",
    "\n",
    "    regs_read, regs_write = disasm[offset].regs_access()\n",
    "    curr_reg_read = set(disasm[offset].reg_name(r) for r in regs_read if disasm[offset].reg_name(r) not in ('eflags', 'rflags'))\n",
    "\n",
    "\n",
    "    # curr_reg_read = set( [ disasm[offset].reg_name(r) for r in disasm[offset].regs_read if disasm[offset].reg_name(r) not in ('eflags', 'rflags')])\n",
    "    if debug:\n",
    "        print( '           curr: red: ' , curr_reg_read)\n",
    "    for prev in preds[offset]:\n",
    "\n",
    "        _ , prev_regs_write = disasm[prev].regs_access()\n",
    "        prev_reg_write = set(disasm[prev].reg_name(r) for r in prev_regs_write if disasm[prev].reg_name(r) not in ('eflags', 'rflags'))\n",
    "        \n",
    "        if debug:\n",
    "            print('           prev: write: ',prev, prev_reg_write ) # if disasm[offset].reg_name(r) not in ('eflags', 'rflags')])\n",
    "        \n",
    "        if prev_reg_write & curr_reg_read:\n",
    "            H_list[prev].append(DEF_USE)\n",
    "            H_list[offset].append(DEF_USE)\n",
    "            \n",
    "            if debug:\n",
    "                print(\"           ----> \", prev)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b06c94f9-dbd7-4612-a9dc-db57e5871476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update_H(H_list, H):\n",
    "    \"\"\" MATH determined from https://www.cs.purdue.edu/homes/zhan3299/res/ICSE19.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    for offset in H_list:\n",
    "        \n",
    "        prod = 1.0\n",
    "        if len(H_list[offset])>0:\n",
    "            for hint in H_list[offset]:\n",
    "                prod = prod * hint\n",
    "            H[offset] = prod\n",
    "        else:\n",
    "            H[offset] = BOTTOM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "048350f4-3ff8-431f-8159-727ce8f78199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import math\n",
    "def safe_product(numbers):\n",
    "    res = None\n",
    "    if len(numbers)==1:\n",
    "        res =  numbers[0]\n",
    "    else:\n",
    "        log_sum = sum(math.log(x) for x in numbers)\n",
    "        res = math.exp(log_sum)\n",
    "    if res == 0:\n",
    "        return 2.2250738585072014e-307\n",
    "    return res\n",
    "    \n",
    "\n",
    "def calc_data_prob(offset ,RH ,H):\n",
    "     \n",
    "    d=1.0\n",
    "    factors = []\n",
    "    for rh in RH[offset]:\n",
    "        factors.append(H[rh])\n",
    "    if len(factors):\n",
    "        d = safe_product(factors)\n",
    "    return d\n",
    "\n",
    "\n",
    "def _forward_propagation(disasm, data_prob, cfg, H , RH,ALL_RD_CFG):\n",
    "    \"\"\" Iterative analyais to find instructions that lead to bad assembly.\n",
    "        Outside of control flow, this is guarenteed to be data\n",
    "\n",
    "        attempting to write ~line 10 algorithm 1 of https://www.cs.purdue.edu/homes/zhan3299/res/ICSE19.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    debug = False\n",
    "    fixed_point = True\n",
    "    for offset, inst in disasm.items():\n",
    "        if data_prob[offset] == 1.0:\n",
    "            # Already know instruction is data\n",
    "            continue\n",
    "        if disasm[offset] is None:\n",
    "            continue\n",
    "\n",
    "        # line 13-15\n",
    "        # update this instructions probability\n",
    "        if H[offset] and offset not in RH[offset]:\n",
    "            RH[offset].add(offset)\n",
    "            \n",
    "            # data_prob[offset] = 1.0\n",
    "            data_prob[offset] = calc_data_prob(offset, RH ,H)\n",
    "            # for h in RH[offset]:\n",
    "            #     data_prob[offset] *= H[h]\n",
    "                \n",
    "            if 0 in data_prob.values():\n",
    "                return\n",
    "        #line 16-20\n",
    "        for n in ALL_RD_CFG[offset]:\n",
    "            diff = set(RH[offset]) - set(RH[n])\n",
    "            if len(diff):\n",
    "                RH[n] = RH[n] | diff\n",
    "                data_prob[n] = calc_data_prob(n, RH ,H)\n",
    "                \n",
    "                if n<offset:\n",
    "                    fixed_point = False\n",
    "    return fixed_point\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687386b-014a-42a3-bfdd-0fb2bc8c7d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c21961b-4a8e-4ffc-8b79-cf269dd2e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import builtins\n",
    "def _adjust_occlusion_probs(disasm, data_prob, occlusion_space):\n",
    "    \"\"\" attempting to write ~line 22 algorithm 1 of https://www.cs.purdue.edu/homes/zhan3299/res/ICSE19.pdf\n",
    "\n",
    "        Struggled with which probabilities should be adjusted and whether that is a global change\n",
    "        or incremental\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    for offset, detail in disasm.items():\n",
    "        if not data_prob[offset] == BOTTOM:\n",
    "            # Only update if data probability is unknown\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Find probability of being data for each overlapping instruction\n",
    "        occluded_probs = []\n",
    "        for j in occlusion_space[offset]:\n",
    "            if data_prob[j] == BOTTOM : #todo nahid hack to prevent zero data prob\n",
    "                continue\n",
    "            occluded_probs.append(data_prob[j])\n",
    "\n",
    "        if len(occluded_probs)==0:\n",
    "            # If not overlapping instructions, leave probability as is\n",
    "            continue\n",
    "\n",
    "        # Step II. In lines 22-24, the algorithm traverses all the addresses\n",
    "        # and performs local propagation of probabilities within occlusion space of individual instructions. Particularly, for each\n",
    "        # address i, it finds its occluded peer j that has the minimal\n",
    "        # probability (i.e., the most likely instruction). The likelihood\n",
    "        # of i being data is hence computed as 1 − D[j] (line 24).\n",
    "        new_occluded_prob = 1 - builtins.min(occluded_probs) # nahid fix*0.9\n",
    "        # print(data_prob[offset] , new_occluded_prob)\n",
    "        if new_occluded_prob==0:\n",
    "            data_prob[offset] = 0.1  #TODO nahid fix todo#2.2250738585072014e-307\n",
    "\n",
    "        else:\n",
    "            data_prob[offset] = new_occluded_prob \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8836042-0de0-41f9-8c5c-074144ed6ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a818061-ce66-4be6-8a9f-16686348775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _back_propagation(disasm, data_prob ,ALL_RD_PRED):\n",
    "    \"\"\" Iterative analysis to find instructions that lead to bad assembly.\n",
    "        Outside of control flow, this is guaranteed to be data.\n",
    "\n",
    "        Attempting to implement ~line 25 of Algorithm 1 from:\n",
    "        https://www.cs.purdue.edu/homes/zhan3299/res/ICSE19.pdf\n",
    "    \"\"\"\n",
    "    debug = False\n",
    "    fixed_point = True\n",
    "    for offset in disasm:\n",
    "        \n",
    "        if data_prob[offset] == BOTTOM :#or offset not in ALL_RD_PRED\n",
    "            # Cannot propagate unknown probability or unknown predecessors\n",
    "            continue\n",
    "       \n",
    "\n",
    "        for p in ALL_RD_PRED[offset]:\n",
    "            # Updated probability propagation logic\n",
    "\n",
    "            if debug:\n",
    "                print('here1',offset , p,data_prob[p] ,data_prob[offset])\n",
    "\n",
    "            if data_prob[p] is BOTTOM or data_prob[p] < data_prob[offset]:\n",
    "                data_prob[p] = data_prob[offset] \n",
    "                fixed_point = False  # Mark that we've updated a probability\n",
    "\n",
    "                if debug:\n",
    "                    print('main', offset ,  'backtracked', p)\n",
    "                if p > offset:\n",
    "                    # Ensure continued processing if new updates occur\n",
    "                    fixed_point = False\n",
    "\n",
    "\n",
    "    return fixed_point\n",
    "\n",
    "# _back_propagation(disasm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a8302d-4b09-46f4-a99f-6bda7890af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_interrupt(instr):\n",
    "    return instr.mnemonic in {'int', 'int3', 'syscall', 'sysenter', 'iret', 'iretq'}\n",
    "def is_nop(instr):\n",
    "    return instr.mnemonic == 'nop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "742c19b3-373e-42e5-994a-8754a51898ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize(disasm,data_prob ,ground_truth_offsets ,occlusion_space, P):\n",
    "\n",
    "    debug = False\n",
    "        \n",
    "    for offset in disasm:\n",
    "        \n",
    "        if disasm[offset] is None:\n",
    "            P[offset] = 0\n",
    "            continue\n",
    "        if data_prob[offset] ==1.0 or data_prob[offset] is None:\n",
    "            P[offset] = 0\n",
    "            continue\n",
    "    \n",
    "        #padding\n",
    "        if is_interrupt(disasm[offset]) or is_nop(disasm[offset]):\n",
    "            P[offset] = 0\n",
    "            if offset in ground_truth_offsets:\n",
    "                ground_truth_offsets.remove(offset)\n",
    "            continue\n",
    "    \n",
    "        # if offset == 3455:\n",
    "        #     debug = True\n",
    "        # else:\n",
    "        #     debug = False\n",
    "        \n",
    "        s=1/data_prob[offset]\n",
    "    \n",
    "        if s == float('inf'):\n",
    "            if debug:\n",
    "                print('infinity')\n",
    "            P[offset] = 1\n",
    "            continue\n",
    "            \n",
    "        if debug:\n",
    "            print('s=1/data_prob[offset]' , s,data_prob[offset])\n",
    "        \n",
    "        for j in occlusion_space[offset]:\n",
    "            if data_prob[j]:\n",
    "                s = s + 1/(data_prob[j] )\n",
    "            else:\n",
    "                pass #todo fix nahid hack\n",
    "            if debug:\n",
    "                print('s = s + 1/(data_prob[j] )', s)\n",
    "        \n",
    "        if debug:\n",
    "            print('final',(1/data_prob[offset]) /s)\n",
    "        final_res = (1/data_prob[offset]) /s\n",
    "        if final_res == float('nan'):\n",
    "            P[offset] = 1 \n",
    "            continue\n",
    "        P[offset] = final_res\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "    for offset, p in P.items():\n",
    "    \n",
    "            if p >.45:\n",
    "                predictions.append(offset)\n",
    "            # else:\n",
    "            #     print(offset, p)\n",
    "    false_positive = set(predictions) -set (ground_truth_offsets)\n",
    "    false_negative = set (ground_truth_offsets) - set(predictions)\n",
    "    true_positive = set (ground_truth_offsets) & set(predictions)\n",
    "    print('false_positive: ',len(false_positive) , ' false_negative: ',len(false_negative) , ' true_positive: ',len(true_positive), 'total:', len(ground_truth_offsets)) \n",
    "\n",
    "    if debug:\n",
    "        for offset in disasm:\n",
    "            # try:\n",
    "            tok = \"   \"\n",
    "            if offset in false_negative:\n",
    "                tok = \"N-N\"\n",
    "            elif offset in false_positive:\n",
    "                tok = \"x  \"\n",
    "            if disasm[offset]:\n",
    "                print( str((P[offset] )).ljust(10),'  ',str( (data_prob[offset] ) ).ljust(10), str(H[offset]).ljust(10) ,str(RH[offset]).ljust(20) ,tok, '#' if offset in ground_truth_offsets else ' ',' ',offset , ' : ', hex(offset),' ',disasm[offset].mnemonic +' ' + disasm[offset].op_str  , ' ' , disasm[offset].size)\n",
    "            else:\n",
    "                print( str((P[offset])).ljust(10),'  ',str( (data_prob[offset] ) ).ljust(10), str(H[offset]).ljust(10) ,str(RH[offset]).ljust(20) , tok,'   ', offset , ' : '    , 'invalid instruction') \n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(traceback.print_exc() )\n",
    "        #     print(e)\n",
    "    return len(false_positive),len(false_negative),len(true_positive),len(ground_truth_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99db84a3-9c55-4e63-802a-fdca87d7ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# false_positive:  112  false_negative:  21  true_positive:  1459 total: 1480\n",
    "# just one one false_positive:  155  false_negative:  46  true_positive:  1434 total: 1480\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c3a23-624f-427c-ac18-4592e2c38ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "381ae63f-1206-49ba-b3ed-0e31083cf12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_sweep(disasm, valid_instructions, ground_truth_offsets):\n",
    "\n",
    "    debug = False\n",
    "    _ = list(valid_instructions)\n",
    "    predictions = _.copy()\n",
    "    for offset in _:\n",
    "        if is_interrupt(disasm[offset]) or is_nop(disasm[offset]):\n",
    "            predictions.remove(offset)\n",
    "    false_positive = set(predictions) -set (ground_truth_offsets)\n",
    "    false_negative = set (ground_truth_offsets) - set(predictions)\n",
    "    true_positive = set (ground_truth_offsets) & set(predictions)\n",
    "    print('false_positive: ',len(false_positive) , ' false_negative: ',len(false_negative) , ' true_positive: ',len(true_positive), 'total:', len(ground_truth_offsets)) \n",
    "    print(predictions.count(0))\n",
    "\n",
    "    if debug:\n",
    "        for offset in disasm:\n",
    "        \n",
    "            # try:\n",
    "                tok = \"   \"\n",
    "                if offset in false_negative:\n",
    "                    tok = \"N-N\"\n",
    "                elif offset in false_positive:\n",
    "                    tok = \"x  \"\n",
    "                if disasm[offset]:\n",
    "                    print( str((P[offset] )).ljust(20),'  ',str( (data_prob[offset] ) ).ljust(20), str(H[offset]).ljust(10) ,str(RH[offset]).ljust(20) ,tok, '#' if offset in ground_truth_offsets else ' ',' ',offset , ' : ', hex(offset),' ',disasm[offset].mnemonic +' ' + disasm[offset].op_str  , ' ' , disasm[offset].size)\n",
    "                else:\n",
    "                    print( str((P[offset] )).ljust(20),'  ',str( (data_prob[offset]) ).ljust(20), str(H[offset]).ljust(10) ,str(RH[offset]).ljust(20) , tok,'   ', offset , ' : '    , 'invalid instruction') \n",
    "        \n",
    "            # except Exception as e:\n",
    "            #     print(traceback.print_exc() )\n",
    "            #     print(e)\n",
    "    return len(false_positive),len(false_negative),len(true_positive),len(ground_truth_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2e203a7-2e9b-4dd7-be14-ec298f20e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_binary(bin_file_path):\n",
    "    print(bin_file_path)\n",
    "# for bin_file_path in bin_files[3:4]:#[1:1000]:\n",
    "\n",
    "    file_name = os.path.basename(bin_file_path)\n",
    "    json_file_path = os.path.join( save_path, file_name + \".json\")\n",
    "    if os.path.exists(json_file_path):\n",
    "        return\n",
    "        \n",
    "    if BIN_FILE_TYPE == \"ELF\":\n",
    "        bin_file_path = os.path.join(bin_path , bin_file_path)\n",
    "    \n",
    "    md = Cs(CS_ARCH_X86, CS_MODE_64)\n",
    "    md.detail = True\n",
    "    offset_inst = {}\n",
    "\n",
    "    \n",
    "    with open(bin_file_path, 'rb') as f:\n",
    "\n",
    "        try:\n",
    "            if BIN_FILE_TYPE == \"ELF\":\n",
    "                elffile = ELFFile(f)\n",
    "                textSection = elffile.get_section_by_name('.text').data()\n",
    "                text_section_offset = elffile.get_section_by_name('.text')['sh_offset']\n",
    "              \n",
    "            elif BIN_FILE_TYPE == \"PE\":\n",
    "                pe_file = lief.parse(bin_file_path)\n",
    "                text_section = pe_file.get_section(\".text\")\n",
    "                text_section_offset = text_section.pointerto_raw_data\n",
    "                textSection = bytes(text_section.content)\n",
    "\n",
    "                \n",
    "            ground_truth_offsets = get_ground_truth_ghidra(bin_file_path, text_section_offset , len(textSection))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e ,bin_file_path)\n",
    "            return\n",
    "\n",
    "    inst_sizes = {}\n",
    "    for byte_index in range(len(textSection)):\n",
    "        try:    \n",
    "\n",
    "            instruction = next(md.disasm(textSection[byte_index: byte_index+15 ], text_section_offset + byte_index ), None)\n",
    "            offset_inst[text_section_offset+byte_index] = instruction\n",
    "            inst_sizes [text_section_offset+byte_index] = instruction.size if instruction else None\n",
    "            \n",
    "            # if instruction:\n",
    "            #     print(\"%d:\\t%s\\t%s _\\t%x\" %(int(instruction.address), instruction.mnemonic, instruction.op_str, instruction.size))\n",
    "            # else:\n",
    "            #     print(\"%d:\\t%s \" % (text_section_offset + byte_index  , 'invalid instruction') )\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(traceback.print_exc() )\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "    \n",
    "    disasm = collections.OrderedDict(sorted(offset_inst.items()))\n",
    "\n",
    "    DATA_OFFSETS = find_data_in_textsection(ground_truth_offsets , text_section_offset , len(textSection) , offset_inst)\n",
    "    code_boundary = text_section_offset+len(textSection)\n",
    "    \n",
    "    ####################### prob disasm calls\n",
    "    \n",
    "    ALL_RD_CFG = {}\n",
    "    H_list = {}\n",
    "    ALL_RD_PRED ={}\n",
    "    res, data_prob,  = {}, {}\n",
    "    \n",
    "    H={}\n",
    "    RH = {}\n",
    "    P = {}\n",
    "\n",
    "\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    cfg, preds = _compute_destinations(disasm)\n",
    "    cfg = dict(sorted(cfg.items()))\n",
    "    preds = dict(sorted(preds.items()))\n",
    "    occlusion_space, valid_instructions =_compute_occlusion(disasm)\n",
    "    get_recursive_descent_cfg(disasm,ALL_RD_CFG )\n",
    "    get_recursive_preds(disasm , ALL_RD_PRED ,ALL_RD_CFG)\n",
    "    initialize(disasm,data_prob,H_list)\n",
    "    \n",
    "    for offset in disasm:\n",
    "        if disasm[offset] is None:\n",
    "            continue\n",
    "        _hint_one_one(offset, disasm, cfg, H_list)\n",
    "        _hint_one(offset, disasm, preds, H_list)\n",
    "        _hint_two(offset, disasm,preds ,H_list)\n",
    "        _hint_three(offset, disasm, preds,H_list)\n",
    "    update_H(H_list, H)\n",
    "    for offset, inst in disasm.items():\n",
    "        RH[offset] = set()\n",
    "    \n",
    "    \n",
    "    for _ in range(MAX_ITERATIONS): #MAX_ITERATIONS\n",
    "        fixed_point = True\n",
    "        fixed_point = _forward_propagation(disasm, data_prob, cfg, H, RH , ALL_RD_CFG) and fixed_point\n",
    "        fixed_point = _adjust_occlusion_probs(disasm, data_prob, occlusion_space) and fixed_point\n",
    "        fixed_point = _back_propagation(disasm ,data_prob , ALL_RD_PRED) and fixed_point\n",
    "        \n",
    "        if fixed_point is True:\n",
    "            break\n",
    "    \n",
    "    prob_disasm_false_positive,prob_disasm_false_negative,prob_disasm_true_positive,prob_disasm_total = normalize(disasm,data_prob ,ground_truth_offsets ,occlusion_space , P)\n",
    "    \n",
    "    linear_sweep_false_positive,linear_sweep_false_negative,linear_sweep_true_positive,linear_sweep_total = linear_sweep(disasm, valid_instructions, ground_truth_offsets)\n",
    "    ################## saving \n",
    "    \n",
    "    \n",
    "    prob_disasm_results = {\n",
    "        \"prob_disasm_false_positive\": prob_disasm_false_positive,\n",
    "        \"prob_disasm_false_negative\": prob_disasm_false_negative,\n",
    "        \"prob_disasm_true_positive\": prob_disasm_true_positive,\n",
    "        \"prob_disasm_total\": prob_disasm_total,\n",
    "        \"linear_sweep_false_positive\": linear_sweep_false_positive,\n",
    "        \"linear_sweep_false_negative\": linear_sweep_false_negative,\n",
    "        \"linear_sweep_true_positive\": linear_sweep_true_positive,\n",
    "        \"linear_sweep_total\": linear_sweep_total\n",
    "        }\n",
    "\n",
    "\n",
    "    \n",
    "    # Save to file\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        json.dump(prob_disasm_results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6d0a82e-397f-4ac4-b6e9-74c008d2d786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raisul/DATA/temp/x86_pe_msvc_O2_static/00624492ea7352748ff174fc34bc7fc6.exe\n"
     ]
    }
   ],
   "source": [
    "process_binary(bin_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6298c32b-6430-4c65-ad24-b156de841aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 192 CPUs on this machine\n",
      "/home/raisul/DATA/temp/x86_pe_msvc_O2_static/00624492ea7352748ff174fc34bc7fc6.exe/home/raisul/DATA/temp/x86_pe_msvc_O2_static/00cb97ceea7b7b7e5edf3148272910c9.exe/home/raisul/DATA/temp/x86_pe_msvc_O2_static/00d2aa1375c72b1d58e77591246702cf.exe/home/raisul/DATA/temp/x86_pe_msvc_O2_static/0087d29b9c2d8670736e624d77673605.exe/home/raisul/DATA/temp/x86_pe_msvc_O2_static/0013a7d72da313f7965b44dc333fe75b.exe\n",
      "\n",
      "/home/raisul/DATA/temp/x86_pe_msvc_O2_static/0007cbb6127f287709e0c460f568916c.exe/home/raisul/DATA/temp/x86_pe_msvc_O2_static/00f1f6a8272c571cb3533c955716669d.exe/home/raisul/DATA/temp/x86_pe_msvc_O2_static/018c3238dac11e2ff081fb05d0220870.exe\n",
      "/home/raisul/DATA/temp/x86_pe_msvc_O2_static/01ade9299269058cd786329d5b6a1518.exe\n",
      "/home/raisul/DATA/temp/x86_pe_msvc_O2_static/021e7e0cf652921f603df7e2790e59bb.exe\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah DONE ALL SUCCESSFULLY Alhamdulillah\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import active_children\n",
    "\n",
    "if __name__ == \"__main__\":  # Allows for the safe importing of the main module\n",
    "    print(\"There are {} CPUs on this machine\".format( multiprocessing.cpu_count()))\n",
    "    \n",
    "    number_processes = int(multiprocessing.cpu_count() *1 )\n",
    "    pool = multiprocessing.Pool(number_processes)\n",
    "\n",
    "\n",
    "    results = pool.map_async(process_binary, bin_files)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    print(\" DONE ALL SUCCESSFULLY Alhamdulillah\"*50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80f8b46e-9354-44ce-92c7-1112a5ed9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to script prob.ipynb\n",
    "# accelerate launch prob.py > log.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
